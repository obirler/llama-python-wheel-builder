This repository contains gihub workflows for the latest version of [python bindings](https://github.com/abetlen/llama-cpp-python "Python bindings for llama.cpp") for [llama.cpp](https://github.com/ggerganov/llama.cpp "LLM inference in C/C++"). llama.cpp is C/C++ implementation of LLM inference. This enables us to run Large Language Models with cpu and gpu altogether.
